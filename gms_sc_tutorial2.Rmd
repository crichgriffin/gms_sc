---
title: "GMS single cell practical 2 - Integration and dimension reduction"
author: "Charlotte Rich-Griffin"
date: "04/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Seurat)
library(SeuratData)
library(cowplot)
```

This tutorial uses the data and loosely follows the tutorial here: https://satijalab.org/seurat/v3.2/immune_alignment.html

This tutorial walks through an alignment of two groups of PBMCs from [Kang et al, 2017.](https://www.nature.com/articles/nbt.4042) In this experiment, PBMCs were split into a stimulated and control group and the stimulated group was treated with interferon beta. The response to interferon caused cell type specific gene expression changes that makes a joint analysis of all the data difficult, with cells clustering both by stimulation condition and by cell type.

# Load the data

```{r}
data(ifnb)
ifnb
```

What does the metadata look like?

```{r}
ifnb@meta.data 
```

In this datase, the two samples are labelled CTRL and STIM. This data set has already been quality controlled but we can take a quick look at the QC metrics.
*Make Violin plots for nCount_RNA and nFeature_RNA*
*Look up in the documentation how to group the plots by different samples*
```{r}
VlnPlot(ifnb, features = c('nCount_RNA', 'nFeature_RNA'),group.by ="stim", pt.size = 0.01)
```

*Make a scatter plot compaing nCount_RNA and nFeature_RNA*
```{r}
FeatureScatter(ifnb, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
```

Based on these plots, are we happy about the filtering that has been done?

Some of the integration steps are quite computationally intensive, so if you were running this in "real life" you will probably need to get access to some High Performance Comuting facilities.

For the pruposes of today, we are going to sample our data down so we can run it locally
```{r}
set.seed(42)
ifnb = subset(ifnb, cells = sample(Cells(ifnb), 4000))
# check we have approximately equal CTRL and STIM cells
table(ifnb@meta.data$stim)
```


# Integrate and Normalise Data

Two options are available from Seurat for integration and noramlisation of data:
- Standard Workflow
- SCTransform

Why integrate data? 

Integration is also referred to as batch correction, as you are essentially trying to merge your samples. What are some potential source of batch effects? How might you mitigate vbatch effects with experimental design?

### Standard Workflows: 
```{r}
ifnb.list <- SplitObject(ifnb, split.by = "stim")

ifnb.list <- lapply(X = ifnb.list, FUN = function(x) {
    x <- NormalizeData(x)
    x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
```
```{r}
immune.anchors <- FindIntegrationAnchors(object.list = ifnb.list, dims = 1:20)
```
```{r}
immune.combined <- IntegrateData(anchorset = immune.anchors, dims = 1:20)

```

```{r}
DefaultAssay(immune.combined) <- "integrated"

```

```{r}
immune.combined <- RunUMAP(immune.combined, reduction = "pca", dims = 1:20)
p1 <- UMAPPlot(immune.combined, group.by="stim" )
```

### SCTransform
The alternative normalisation method SCTransform can be used to do this integration using the following code, maybe try it out in your own time.
The method and it's benefits are discussed in the [preprint](https://www.biorxiv.org/content/10.1101/576827v2) and a separate [vignette](https://satijalab.org/seurat/v3.0/sctransform_vignette.html)

```{r}
# for (i in 1:length(ifnb.list)) {
#     ifnb.list[[i]] <- SCTransform(ifnb.list[[i]], verbose = FALSE)
# }
# immune.features <- SelectIntegrationFeatures(object.list = ifnb.list, nfeatures = 3000)
# immune.list <- PrepSCTIntegration(object.list = ifnb.list, anchor.features = immune.features,
#     verbose = FALSE)
# 
# immune.anchors.sct <- FindIntegrationAnchors(object.list = immune.list, dims = 1:20,	anchor.features = immune.features, normalization.method = "SCT")
# 
# immune.combined.sct <- IntegrateData(anchorset = immune.anchors, normalization.method = "SCT",
#     verbose = FALSE)
```

## Other methods

There are also a host of other methods, one of the most popular current tools is Harmony
[github](https://github.com/immunogenomics/harmony)
[paper](https://www.nature.com/articles/s41592-019-0619-0)

One last comment about integration before we move on. Lots of different methods work in different ways! Different methods work better on different datasets. Methods of benchmarking different batch correction/integration techniques are discussed in this [paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1850-9)

The main thing I would recommend is always look at your dimension reduction plots to compare how different methods are acting on your dataset.

For our dataset we can compare what the UMAP would look like with and without integration
```{r}
# quickly run the prerequisite steps for UMAP on our original data
ifnb <- FindVariableFeatures(ifnb, nfeatures = 2000)
ifnb <- ScaleData(ifnb)
ifnb <- RunPCA(ifnb)
ifnb <- RunUMAP(ifnb, reduction = "pca", dims=1:50)
```


```{r,fig.width=8, fig.height=3}
# no integration
p1 <- UMAPPlot(ifnb, group.by="stim" )
# Integrated
p2 <- UMAPPlot(immune.combined, group.by="stim")
plot_grid(p1, p2)
```

# Dimension reduction and clustering

Having integrated our dataset, we can get back to trying to visualise our data set 

```{r}
# Run the standard workflow for visualization and clustering
immune.combined <- ScaleData(immune.combined, verbose = FALSE)
immune.combined <- RunPCA(immune.combined, npcs = 30, verbose = FALSE)

```
# t-SNE and Clustering

```{r}
immune.combined <- RunUMAP(immune.combined, reduction = "pca", dims = 1:20)
immune.combined <- FindNeighbors(immune.combined, reduction = "pca", dims = 1:20)
immune.combined <- FindClusters(immune.combined, resolution = 0.5)
UMAPPlot(immune.combined)
```
## A word about Parameters

But what a parameters are the "best" parameters? 
Answer :shrug_emoji: it depends!! 

Every dataset is different.

Let's save our data set here
```{r}
saveRDS(immune.combined, file = "~/gms/data/ifnb_integrated.rds")
```

